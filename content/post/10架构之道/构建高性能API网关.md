# 构建高性能API网关 #

API网关的功能：

反向代理
安全认证
限流熔断
日志监控

## 1. 高并发调用模型 ##

同步调用受限于线程数量，而线程资源宝贵，在API网关这类高并发应用场景下，可以采用异步化的引入彻底的隔离API之间的影响。网关在Servlet线程在进行完API调用前置校验后，使用 HSF或HTTP NIO client发起远程服务调用，并结束和回收到该线程。待HSF或者HTTP请求得到响应后，以事件驱动的方式将远程调用响应结果和API请求上下文信息，提交到TOP工作线程池，由TOP工作线程完成后续的数据处理。最后使用Jetty Continuation特性唤起请求将响应结果输出给调用方，实现请求的全异步化处理。

Servlet 3.0 作为 Java EE 6 规范体系中一员，随着 Java EE 6 规范一起发布。该版本在前一版本（Servlet 2.5）的基础上提供了若干新特性用于简化 Web 应用的开发和部署（Tomcat7提供了对Java EE6规范的支持。）。

在Servlet 3.0中，我们可以从HttpServletRequest对象中获得一个AsyncContext对象，该对象构成了异步处理的上下文，Request和Response对象都可从中获取。AsyncContext可以从当前线程传给另外的线程，并在新的线程中完成对请求的处理并返回结果给客户端，初始线程便可以还回给容器线程池以处理更多的请求。如此，通过将请求从一个线程传给另一个线程处理的过程便构成了Servlet 3.0中的异步处理。Servlet 3.0对请求的处理虽然是异步的，但是对InputStream和OutputStream的IO操作却依然是阻塞的，对于数据量大的请求体或者返回体，阻塞IO也将导致不必要的等待。因此在Servlet 3.1中引入了非阻塞IO。

* 声明Servlet，注意增加asyncSupported的属性，开启异步处理支持。@WebServlet(urlPatterns = "/demo", asyncSupported = true)
* 在Servlet内部，需要独立线程处理的地方，使用request获取异步Context。AsyncContext ctx = req.startAsync();
* 在独立线程中，使用异步Context，可以获取到其绑定的request和response，此时，就可以按照原来Servlet的写法，继续编写逻辑了。
* 独立线程内的操作处理完成后，需要调用异步Context的complet方法，来结束该异步线程。

需要注意的是，异步Servlet有对应的超时时间，如果在指定的时间内没有执行完操作，response依然会走原来Servlet的结束逻辑，后续的异步操作执行完再写回的时候，可能会遇到异常。

使用Golang重写网关，依赖协程处理高并发调用。

## 2. 元数据多级缓存 ##

在API调用链路中会依赖对元数据的获取，比如需要获取API、APP、鉴权信息等等。在大量数据场景下，元数据获取QPS高达上千万，如何优化元数据获取的性能是API网关的关键点。

千万级QPS全部打到DB是不可取的，考虑到元数据实时性没这么高，所以可以采用定时刷新到内存的方式。当内存中存放不下时，需要增加一层分布式缓存;然而千万级 QPS需要近百台缓存服务器，为了节约缓存服务器开销以及减少过多的网络请求，我们在分布式缓存前面加了一层LRU 规则的本地缓存;为了防止缓存被击穿，我们在本地缓存前面加了一层BloomFilter。一套基于漏斗模型的元数据读取架 构产生(如图2所示)。缓存控制中心可以动态推送缓存规则，如数据是否进行缓存、缓存时长、本地缓存大小。为了解 决缓存数据过期时在极端情况下可能出现的并发请求问题，网关会容忍拿到过期的元数据(多数情况对数据时效性要求 不高)，并提交异步任务更新数据信息。

## 3. 批量API ##

在双11高并发的场景下,对商家和ISV的系统同样是一个考验,如何提高ISV请求API的性能,降低请求RT和网络消耗同 样是一个重要的事情。在ISV开发的系统中通常存在这样的逻辑单元,需要调用多个API才能完成某项业务(如图3),在 这种串行调用模式下RT较长同时多次调用发送较多重复的报文导致网络消耗过多,在弱网环境下表现更加明显。

API网关提供批量API调用模式(如图4所示)缓解ISV在调用RT过高和网络消耗上的痛点。ISV发起的批量请求会在TOP SDK进行合并,并发送到指定的网关;网关接收到请求后在单线程模式下进行公共逻辑计算,计算通过后将调用安装API 维度拆分,并分别发起异步化远程调用,至此该线程结束并被回收;每个子API的远程请求结果返回时会拿到一个线程进 行私有逻辑处理,处理结束时会将处理结果缓存并将完成计数器加一;最后完成处理的线程,会将结果进行排序合并和 输出。

## 4. 监控体系 ##

**1.实时数据**

uuid
module_time
app_id
api_url
msg_id
return_code
start_time
end_time
consume_time
localhost
remotehost

**2.非实时数据**

全量交易日志，ES日志收集。



**1.系统维度**

每台服务器的CPU、IO、Memory等。

**2.APP维度**

该APP在每分钟/小时/天内的调用数、失败数、超时数，平均耗时，平均连接数，qps。

该APP对不同API在每分钟/小时/天内的调用数、失败数、超时数，平均耗时，平均连接数，qps。

**3.API维度**

该API在每分钟/小时/天内的调用数、失败数、超时数，平均耗时，平均连接数，qps。

该API对不同APP在每分钟/小时/天内的调用数、失败数、超时数，平均耗时，平均连接数，qps。

## 限流 ##

TOP API网关暴露在互联网环境,日调用量达几百亿。特别是在双11场景中,API调用基数大、调用者众多以及各个API 的服务能力不一致,为了保证各个API能够稳定提供服务,不会被暴涨的请求流量击垮,那么多维度流量控制是API网关 的一个重要环节。API网关提供一系列通用的流量控制规则,如API每秒流控、API单日调用量控制、APPKEY单日调用量

控制等。
在双11场景中,也会有一些特殊的流量控制场景,比如单个API提供的能力有限,例如只能提供20万QPS的能力而实际 的调用需求可能会有40万QPS。在这种场景下怎么去做好流量分配,保证核心业务调用不被限流。TOP API网关提供了 流量分组的策略,比如我们可以把20万QPS的能力分为3个组别,并可以动态去配置和调整每个组别的比例,如:分组1 占比50%、如分组2占比40%、分组3占比10%。我们将核心重要的调用放到分组1,将实时性要求高的调用放到分组2, 将一些实时性要求不高的调用放到分组3。通过该模式我们能够让一些核心或者实时性要求高的调用能够较高概率通过流 量限制获取到相应的数据。同时TOP API网关是一个插件化的网关,我们可以编写流控插件并动态部署到网关,在流控 插件中我们可以获取到调用上下文信息,通过Groovy脚本或简单表达式编写自定义流控规则,以满足双11场景中丰富的 流控场景。

使用集群流控还是单机流控?单机流控的优势是系统开销较小,但是存在如下短板:
1. 集群单机流量分配不均。
2. 单日流控计数器在某台服务器挂掉或者重启时比较难处理。 3. API QPS限制小于网关集群机器数量时,单机流控无法配置。
基于这些问题,API网关最开始统一使用集群流控方案,但在双11前压测中发现如下一些问题:
1. 单KEY热点问题,当单KEY QPS超过几十万时,单台缓存服务器RT明显增加。 2. 缓存集群QPS达到数百万时,服务器投入较高。
针对第一个问题的解法是,将缓存KEY进行分片可将请求离散多台缓存服务器。针对第二个问题,API网关采取了单机 +集群流控相结合的解决方案,对于高QPS API流控采取单机流控方案,服务端使用Google ConcurrentLinkedHashMap缓存计数器,在并发安全的前提下保持了较高的性能,同时能做到LRU策略淘汰过期数 据。

集群流控还是单机流控?流量分组策略？

**1.总体限流**

网关总体连接数，qps。绝对值+百分比

**2.APP限流**

某个APP的连接数，qps。绝对值+百分比

**3.API限流**

某个API的连接数，qps。绝对值+百分比

**4.APP-API限流**

某个APP对某个API调用连接数，qps。绝对值+百分比

## 蓝绿部署 ##

## 参考 ##

《5.3 千亿访问量下的开放平台技术揭秘》，风胜
